<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>susurrant.org: blog</title>
    <link href="http://susurrant.org//atom.xml" rel="self" />
    <link href="http://susurrant.org/" />
    <id>http://susurrant.org//atom.xml</id>
    <author>
        <name>Cora Johnson-Roberson</name>
        <email>info@susurrant.org</email>
    </author>
    <updated>2015-06-18T00:00:00Z</updated>
    <entry>
    <title>A Visual Analogue for "Algorithmic Listening"</title>
    <link href="http://susurrant.org//posts/2015/06/18/visual-analogue/" />
    <id>http://susurrant.org//posts/2015/06/18/visual-analogue/</id>
    <published>2015-06-18T00:00:00Z</published>
    <updated>2015-06-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[<article>
  <header>
    <h1 class="entry-title">
      <a href="/posts/2015/06/18/visual-analogue/">A Visual Analogue for "Algorithmic Listening"</a>
    </h1>
    <div class="meta small">
      <time datetime="2015-06-18" pubdate data-updated="true">June 18, 2015</time>
      <div class="tags"><a href="/tags/neural%20networks/">neural networks</a> | <a href="/tags/computer%20vision/">computer vision</a></div>
    </div>
  </header>
  <div class="post-content">
    <p>
There’s a fun post over on Google’s Research Blog detailing the <a href="http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html">“dreams” of neural networks</a>, generated by feeding random noise images into the networks and ‘nudging’ the image iteratively toward a certain classification. While the details are somewhat different, this is a great visual analogue for the processes used in Susurrant to expose the workings of its own audio “interpretation” algorithms.
</p>
<!-- MORE -->
<figure>
<img src="/assets/noise-to-banana.png" alt="Seeing like a network (source: Google Research)" />
<figcaption>
Seeing like a network (source: Google Research)
</figcaption>
</figure>
<p>
I saw some folks responding to that article with a desire to apply the technique to audio, which is an integral part of what this tool aims to accomplish.
</p>
<p>
I hope soon to supplement the Susurrant demo with a narrative explanation of what each topic “sounds like,” both to me and the machine.
</p>
<div class="references">

</div>

  </div>
</article>

]]></summary>
</entry>
<entry>
    <title>Presentation at "Inertia" (UCLA)</title>
    <link href="http://susurrant.org//posts/2015/05/03/inertia/" />
    <id>http://susurrant.org//posts/2015/05/03/inertia/</id>
    <published>2015-05-03T00:00:00Z</published>
    <updated>2015-05-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[<article>
  <header>
    <h1 class="entry-title">
      <a href="/posts/2015/05/03/inertia/">Presentation at "Inertia" (UCLA)</a>
    </h1>
    <div class="meta small">
      <time datetime="2015-05-03" pubdate data-updated="true">May  3, 2015</time>
      <div class="tags"><a href="/tags/conference/">conference</a></div>
    </div>
  </header>
  <div class="post-content">
    <p>
I’ve just returned from a wonderful conference at UCLA called “Inertia: A Conference on Sound, Media, and the Digital Humanities,” organized by Mike D’Errico and company. On Friday, I presented the notion of “algorithmic listening” (i.e., the practice of listening to algorithms that listen) and its application in the context of my development of Susurrant.
</p>
<!-- MORE -->
<p>
I greatly enjoyed hearing about the HiPSTAS platform (High Performance Sound Technologies for Access and Scholarship) from its PI Tanya Clement, along with colleagues Marit MacArthur and Eric Rettberg. The approach of that platform is quite different in some respects, and each of their projects gave me a great deal to think about in terms of the kinds of questions that other scholars hope to address using such tools.
</p>
<p>
I’m grateful for the generous and welcoming audience at UCLA, and for the diversity of approaches and ideas I encountered there. I look forward to presenting my work in revised form at the <a href="http://dh2015.org">Global Digital Humanities</a> conference in Sydney this July.
</p>
<div class="references">

</div>

  </div>
</article>

]]></summary>
</entry>

</feed>
